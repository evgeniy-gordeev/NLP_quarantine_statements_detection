{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cotangentofzero/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/cotangentofzero/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import re, string\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath_data,filepath_test):\n",
    "      \n",
    "    #train\n",
    "    data = pd.read_csv(filepath_data, sep='\\t')\n",
    "    data = data[['text', f'{class_name}_{problem_name}']]\n",
    "    data[f'{class_name}_{problem_name}'].replace([-1,0,1,2],[0,1,2,3], inplace = True)\n",
    "    \n",
    "    #test\n",
    "    test = pd.read_csv(filepath_test, sep='\\t')\n",
    "    test = test['text']\n",
    "    \n",
    "    return data,test\n",
    "\n",
    "def preprocess_s1(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]',' ',str(text).lower().strip())\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    text_new = text.strip()\n",
    "    \n",
    "    return text_new\n",
    " \n",
    "def lematize(text):\n",
    "    \n",
    "    text_new = [morph.parse(word)[0].normal_form for word in word_tokenize(text)]\n",
    "    text_new = ' '.join(text_new)\n",
    "    \n",
    "    return text_new\n",
    "\n",
    "def stopword(text):\n",
    "    \n",
    "    stopwords_ = stopwords.words('russian')\n",
    "    extra_stopwords_ = ['че','ниче','изза','user','такой','привет','всё','проклятый']\n",
    "    drop_stopwords_ = ['им','не','по','про','без','надо','было']\n",
    "    stopwords_ = stopwords_ + extra_stopwords_\n",
    "    stopwords_ = [word for word in stopwords_ if word not in drop_stopwords_]\n",
    "    text_new = [word for word in text.split(' ') if word not in stopwords_]\n",
    "    text_new = ' '.join(text_new)\n",
    "    \n",
    "    return text_new\n",
    "\n",
    "def preprocess_s2(text):\n",
    "    \n",
    "    text = re.sub(r'%', ' процент ', text)\n",
    "    text = re.sub(r'счёт', ' счет ', text)\n",
    "    text = re.sub(r'за([ёе])м', ' займ ', text)\n",
    "    text = re.sub(r'юридическ[а-я]{2,3}\\s*лиц([а-я])?', ' юл ', text)\n",
    "    text = re.sub(r'физическ[а-я]{2,3}\\s*лиц([а-я])?', ' физлицо ', text)\n",
    "    text = re.sub(r'э/э', ' электроэнергия ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])(согл(\\.)?|([сc])[\\-\\s+]но)(?![а-яёa-z])', ' согласно ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])вып(олн)?(\\.)', ' выполнение ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])соглаш(\\.|[а-я]*)(?![а-яёa-z])', ' соглашение ', text)\n",
    "    text = re.sub(r'опл\\.дз', ' оплата договор займа ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])(о|o|у|до)пл(\\.|[а-я]*)?(?![а-яёa-z])', ' оплата ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])плата([а-я]*)?(?![а-яёa-z])', ' оплата ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])выплата', ' выплата ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])выпл(\\.|[а-яёa-z]*)?(?![а-яёa-z])', ' выплата ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])у( )?слов([а-яёa-z]*)?(?![а-яёa-z])', ' условия ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])(у( )?сл(\\.)?|(у)?слуг([а-яёa-z]*)?|услу( )ги)(?![а-яёa-z])', ' услуги ', text)\n",
    "    text = re.sub(r'улуги|усуги|усгуги|улуг|усулги|усоуг(и)?|улсуги|усдуги|([кц])слуги|учлуг', ' услуги ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])дог(\\s+)овор([а-яёa-z]*)?(?![а-яёa-z])', ' договор ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])д( )?о( )?г(\\.|[а-яёa-z]*)?(?![а-яёa-z])', ' договор ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])[длп]( )?(о(р)?)?( )?(г)?о( )?в([а-яёa-z]*)?(?![а-яёa-z])', ' договор ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])(в)?( )?о(з)?мещ([а-я]*)?(?![а-яёa-z])', ' возмещение ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])возм(\\.)', ' возмещение ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])откр(\\.)?(?![а-яёa-z])', ' открытый ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])соц(\\.)?(?![а-яёa-z])', ' социальный ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])стр( )?(а)?х([а-я]*)?(?![а-яёa-z])', ' страхование ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])(влзврат|возварат)(?![а-яёa-z])', ' возврат ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])больнич([а-я]*)?(?![а-яёa-z])', ' больничныйлист больничный ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])б/л([а-я]*)?(?![а-яёa-z])', ' больничныйлист больничный ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])пособ(\\.|[а-яёa-z]*)?(?![а-яёa-z])', ' пособие ', text)\n",
    "    text = re.sub(r'мат(\\.)(\\s+)?помо([щш])[а-я](?![а-яёa-z])', ' матпомощь ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])матер[а-я]*(\\s+)?пом[а-я]*(?![а-яёa-z])', ' матпомощь ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])расч(\\.)?(?![а-яёa-z])', ' расчет ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])ра(\\s+)?счет(?![а-яёa-z])', ' расчет ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])поступ[а-яёa-z]*(?![а-яёa-z])', ' поступление ', text)\n",
    "    text = re.sub(r'расх(\\.)', ' расходы ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])расход([а-я]*)?(?![а-яёa-z])', ' расходы ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])размещ(\\.)', ' размещение ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])размещ[а-я]*(?![а-яёa-z])', ' размещение ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])заявл(\\.|[а-я]*)?(?![а-яёa-z])', ' заявление ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])([сc])умм[аеуыоймих]*(?![а-яёa-z])', ' сумма ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])([а-яёa-z]*)?сумм[аеуыоймих]*(?![а-яёa-z])', ' сумма ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])сумм([а-яёa-z]*)?(?![а-яёa-z])', ' сумма ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])[а-яёa-z]{2}куп([а-яёa-z]*)?(?![а-яёa-z])', ' покупка ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])валют[аыеу](?![а-яёa-z])', ' валюта ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])ср[\\-\\s]*в(а)?(?![а-яёa-z])', ' средства ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])сотрудник[а-я]*(?![а-яёa-z])', ' сотрудник ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])сотр(уд(н)?)?(\\.)?(?![а-яёa-z])', ' сотрудник ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])работник[а-я]*(?![а-яёa-z])', ' работник ', text)\n",
    "    text = re.sub(r'раб(отн)?(\\.)', ' работник ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])раб(\\s+)?([-])(\\s+)?[а-я]*(?![а-яёa-z])', ' работник ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])насел(\\.|[а-я]*)?(?![а-яёa-z])', ' население ', text)\n",
    "    text = re.sub(r'имущ\\.', ' имущество ', text)\n",
    "    text = re.sub(r'(?<![а-яёa-z])(мобиль[а-яёa-z.]*|модуль[а-яёa-z.]*)(\\s+)?(здан[а-яёa-z]*|сооруж[а-яёa-z.]*)(?![а-яёa-z])',\n",
    "                  ' оборудование средство ', text)\n",
    "    text = re.sub(r'пост\\.(\\s+)?обор\\.', ' поставка оборудование ', text)\n",
    "    text = re.sub(r'локдаун', ' карантин ', text)\n",
    "    text = re.sub(r'намордник', ' маска ', text)\n",
    "    text = re.sub(r'cпутник', ' вакцина ', text)\n",
    "    text = re.sub(r'прививка', ' вакцина ', text)\n",
    "    text = re.sub(r'вакцинация', ' вакцина ', text)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    text_new = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def replace_dates(text):\n",
    "    \n",
    "    text_months = ['январ[ьяюе]', 'феврал[ьяюе]', 'март[аеу]{0,1}', 'апрел[ьяюе]', 'ма[йеюя]', 'июн[ьяюе]',\n",
    "                   'июл[ьяюе]', 'август[аеу]{0,1}', 'сентябр[ьяюе]', 'октябр[ьяюе]', 'ноябр[ьяюе]', 'декабр[ьяюе]']\n",
    "    text_month_pattern = '|'.join(['(?<![а-я]){}(?![а-я])'.format(text_month) for text_month in text_months])\n",
    "\n",
    "    possible_years1 = ['0[0-9]', '1[0-9]', '[8-9][0-9]', '2[01]']\n",
    "    possible_years2 = ['20', '19', '']\n",
    "    possible_years3 = ['20', '19']\n",
    "    possible_months = ['0[1-9]', '1[0-2]']\n",
    "    possible_days = ['[1-2][0-9]', '3[0-1]', '0{0,1}[1-9]']\n",
    "    possible_seps = [r'\\.', r'\\-', r'\\,', r'\\/']\n",
    "    numeric_date_patterns = [r'(?<![0-9]){0}{4}{1}{5}{3}{2}(?![0-9])'.format(d, m, y1, y2, sep1, sep2)\n",
    "                             for d, m, y1, y2, sep1, sep2 in\n",
    "                             product(possible_days, possible_months, possible_years1, possible_years2, possible_seps,\n",
    "                                     possible_seps)]\n",
    "    text_date_patterns = [r'(?<![0-9])({0})?\\s+месяцмесяц\\s+{2}{1}(?![0-9])'.format(d, y1, y3)\n",
    "                          for d, y1, y3 in product(possible_days, possible_years1, possible_years3)]\n",
    "    date_pattern = '|'.join(numeric_date_patterns + text_date_patterns)\n",
    "    text = re.sub(text_month_pattern, ' месяцмесяц ', text)\n",
    "    text = re.sub(date_pattern, ' датадата ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_data(data,test):\n",
    "        \n",
    "    #train\n",
    "    data['text'] = data['text'].apply(preprocess_s1)\n",
    "    data['text'] = data['text'].apply(lematize)\n",
    "    data['text'] = data['text'].apply(stopword)\n",
    "    data['text'] = data['text'].apply(preprocess_s2)\n",
    "    data['text'] = data['text'].apply(replace_dates)\n",
    "    \n",
    "    #test\n",
    "    test = test.apply(preprocess_s1)\n",
    "    test = test.apply(lematize)\n",
    "    test = test.apply(stopword)\n",
    "    test = test.apply(preprocess_s2)\n",
    "    test = test.apply(replace_dates)\n",
    "    \n",
    "    return data,test\n",
    "\n",
    "def vercorize_data(data,test):\n",
    "        \n",
    "    #data\n",
    "    X_train = data['text']\n",
    "    y_train = data[f'{class_name}_{problem_name}']\n",
    "    X_test = test\n",
    "    \n",
    "    #vectorizer fit\n",
    "    vectorizer = TfidfVectorizer(use_idf=True, max_features = 2000)\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    X_train = pd.DataFrame.sparse.from_spmatrix(X_train, columns = vectorizer.get_feature_names())\n",
    "    X_test = pd.DataFrame.sparse.from_spmatrix(X_test, columns = vectorizer.get_feature_names())\n",
    "        \n",
    "    #result\n",
    "    data = pd.concat([X_train,y_train], axis = 1)\n",
    "    test = X_test\n",
    "        \n",
    "    return data,test\n",
    "        \n",
    "def train_model(data_init,data,test_init,test):\n",
    "    \n",
    "    #fit train\n",
    "    X_train = data.drop(f'{class_name}_{problem_name}',axis=1)\n",
    "    y_train = data[f'{class_name}_{problem_name}']\n",
    "    clf = LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2', random_state=42)\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    #predict test\n",
    "    X_test = test\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    \n",
    "    #result test\n",
    "    results_test = pd.DataFrame({f'{class_name}_{problem_name}':y_test_pred},\n",
    "                                 index = test_init.loc[X_test.index])\n",
    "    \n",
    "    return results_test\n",
    "\n",
    "def make_prediction(data_init,data,test_init,test):\n",
    "    \n",
    "    data,test = vercorize_data(data,test)\n",
    "    results_test = train_model(data_init,data,test_init,test)\n",
    "    \n",
    "    return results_test\n",
    "\n",
    "def pipeline(filepath_data,filepath_test):\n",
    "    \n",
    "    data_init,test_init = read_data(filepath_data,filepath_test)\n",
    "    data,test = process_data(data_init,test_init)\n",
    "    results_test = make_prediction(data_init,data,test_init,test)\n",
    "    \n",
    "    return results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 45s, sys: 2.85 s, total: 5min 47s\n",
      "Wall time: 5min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#read data\n",
    "filepath_data = 'train.tsv'\n",
    "filepath_test = 'test.tsv'\n",
    "class_names = ['masks','quarantine','vaccines']\n",
    "problem_names = ['stance','argument']\n",
    "\n",
    "#pipeline\n",
    "results_test_container = []\n",
    "for class_name in class_names:\n",
    "    for problem_name in problem_names:\n",
    "        results_test_container.append(pipeline(filepath_data,filepath_test))\n",
    "\n",
    "results_test_final = pd.concat(results_test_container,axis = 1)\n",
    "results_test_final.replace([0,1,2,3],[-1,0,1,2],inplace = True)\n",
    "\n",
    "#save result\n",
    "results_test_final.to_csv('result.tsv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda",
   "language": "python",
   "name": "conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
